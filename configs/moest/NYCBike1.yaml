## global
seed:  31  # 11 21 31 41 51
device: cuda
mode: train
best_path: Null  #experiments/NYCBike1/MoESoft/graphormer/w_ceS/best_model.pth   # Null 
debug: False  

## data 
data_dir: data
dataset: NYCBike1  
input_length: 19 # 4+5*3
batch_size: 32
test_batch_size: 32
graph_file: data/NYCBike1/adj_mx.npz # 128 x 128
num_nodes: 128

## Frequency
fft_status: False

## MoE
moe_status: SoftMoE # SoftMoE / MoE / SharedMoE / STMoE / None
num_experts: 6
moe_dropout: 0.1
top_k: 1
moe_mlr: True  # important #
moe_add_ff: False # param 4 STMoE
moe_position: 'Full'  # Full / Half / woS / woT / woST
expertWeightsAda: False # param 4 SharedMoE
expertWeights: [0.8, 0.2] # param 4 SharedMoE

## model 
layers: ['S','T','S','T'] # ['T','S','T','T','S','T'] ['T','S','T','S','T','S'] ['S','T','S','T','S','T']
d_input: 2                  # means inflow and outflow
d_output: 2                 # means inflow and outflow
d_model: 64
num_heads: 4
mlp_ratio: 4
layer_depth: 1    # depth for one S/T Attention Module
dropout: 0.1
yita: 0.6                   # balance for inflow loss and outflow loss, $yita * inflow + (1 - yita) * outflow$
## spatio-temporal information injection
attn_mask_S: False
attn_mask_T: False
attn_bias_S: False
attn_bias_T: False # 没有设置attn_bias_T 
pos_embed_T: True
cen_embed_S: False # Spatial Centrality Encoding

## train
epochs: 250
## learning rate ##
lr_init: 2.0e-3
scheduler: 'MultiStepLR' # StepLR MultiStepLR ExponentialLR ReduceLROnPlateau(效果一般)
step_size: 30 # StepLR
milestones: [1, 60, 90, 120, 150] # MultiStepLR
factor: 0.8 # ReduceLROnPlateau
patience: 10 # ReduceLROnPlateau
gamma: 0.5 # StepLR MultiStepLR ExponentialLR
# 1. MultiStepLR 2.0e-3 [1, 60, 90, 120, 150] 0.5
# 2. MultiStepLR 2.0e-3 [1, 50, 80, 100, 120] 0.5 相比1，MAE差，MAPE好些
# 3. MultiStepLR 1.0e-4 [120, 140, 160, 180, 200] 0.9
## learning rate ##
## mask value setting ##
mask_value_train: 5.0
mask_value_test: 5.0 # 5.0 | Null $important$
## mask value setting ##
early_stop: True
early_stop_patience: 25
grad_norm: True
max_grad_norm: 5
use_dwa: False         # whether to use dwa for loss balance
temp: 4               # tempurature parameter in dwa, a larger T means more similer weights
