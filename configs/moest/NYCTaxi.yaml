## global
seed: 31 # 11 21 31 41 51
device: cuda
mode: train
best_path: Null 
debug: False 

## data
data_dir: data
dataset: NYCTaxi 
input_length: 35
batch_size: 32
test_batch_size: 32
graph_file: data/NYCTaxi/adj_mx.npz # num_nodes 200 (20 x 10)

## MoE
moe_status: 'SharedMoE' # MoE / SharedMoE / STMoE / None
num_experts: 8
moe_dropout: 0.1
top_k: 1
moe_mlr: True  # important #
moe_add_ff: False # param 4 STMoE
moe_position: 'Full'  # Full / Half / woS / woT / woST
expertWeightsAda: False # param 4 SharedMoE
expertWeights: [0.8, 0.2] # param 4 SharedMoE

## model 
d_input: 2                  # means inflow and outflow
d_output: 2                 # means inflow and outflow
d_model: 64
num_heads: 4
mlp_ratio: 4
encoder_depth: 1
dropout: 0.1
yita: 0.5                   # balance for inflow loss and outflow loss, $yita * inflow + (1 - yita) * outflow$
## Sparse ScaledDotProduct Attention
sparse: False
sparseK: Null

## train 
epochs: 250
## learning rate ##
lr_init: 2.0e-3
scheduler: 'MultiStepLR' # StepLR MultiStepLR ExponentialLR ReduceLROnPlateau(效果一般)
step_size: 30 # StepLR
milestones: [1, 60, 90, 120, 150] # MultiStepLR
factor: 0.8 # ReduceLROnPlateau
patience: 10 # ReduceLROnPlateau
gamma: 0.5 # StepLR MultiStepLR ExponentialLR
# 1. MultiStepLR 2.0e-3 [1, 60, 90, 120, 150] 0.5
# 2. MultiStepLR 2.0e-3 [1, 50, 80, 100, 120] 0.5 相比1，MAE差，MAPE好些
# 3. MultiStepLR 1.0e-4 [120, 140, 160, 180, 200] 0.9
## learning rate ##
early_stop: True
early_stop_patience: 25
grad_norm: True
max_grad_norm: 5
use_dwa: False         # whether to use dwa for loss balance
temp: 2               # tempurature parameter in dwa, a larger T means more similer weights
