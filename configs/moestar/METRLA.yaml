## global
seed: 31
device: cuda
mode: train
best_path: Null  #experiments/***/best_model.pth  # Null 
debug: False  

## data 
data_dir: data
dataset: METRLA  
input_length: 12
output_length: 12
batch_size: 32
test_batch_size: 32
graph_file: data/METRLA/adj_mx.npz
num_nodes: 207
num_timestamps: 2016 # 12*24*7
tod_scaler: 288
steps_per_day: 288

## spatio-temporal information injection
layers: ['S','T'] # ['S','T','S','T','S','T'] ['T','S','T','S','T','S'] ['T','T','T','S','S','S'] ['S','S','S','T','T','T'] 
layer_depth: 3    # depth for one S/T Attention Module
pos_embed_T: True
cen_embed_S: True # Spatial Centrality Encoding
attn_bias_S: True
attn_mask_S: False
attn_mask_T: False
## MoE
moe_status: SoftMoE # SoftMoE / SharedMoE / MoE / STMoE / None
moe_mlr: False  # learning rate of experts
num_experts: 6
moe_dropout: 0.1
top_k: 1
moe_add_ff: False # STMoE
moe_position: Full  # Full / Half / woS / woT / woST
expertWeightsAda: False # SharedMoE
expertWeights: [0.8, 0.2] # SharedMoE

## model 
d_input: 3  # speed tod dow
d_output: 1  # speed
d_model: 64
d_time_embed: 24
d_space_embed: 24
num_heads: 4
mlp_ratio: 4
dropout: 0.1
yita: 0.5 # balance for inflow loss and outflow loss, $yita * inflow + (1 - yita) * outflow$
fft_status: False # Frequency

## train
epochs: 200
# learning rate #
lr_init: 0.001
scheduler: StepLR # StepLR MultiStepLR ExponentialLR ReduceLROnPlateau
step_size: 25 # StepLR
milestones: [1, 60, 90, 120, 150] # MultiStepLR
factor: 0.8 # ReduceLROnPlateau
patience: 10 # ReduceLROnPlateau
gamma: 0.5 # StepLR MultiStepLR ExponentialLR
# mask value setting #
mask_value_train: 0.0
mask_value_test: 0.0
# others #
early_stop: True
early_stop_patience: 30
grad_norm: True
max_grad_norm: 5
use_dwa: False # whether to use dwa for loss balance
temp: 4 # tempurature parameter in dwa, a larger T means more similer weights
